1) https://github.com/huggingface/pytorch-transformers 

Summary of the above as found on Reddit:

The library is comprised of six architectures:


    - Google's BERT,
    - OpenAI's GPT,
    - OpenAI's GPT-2,
    - Google/CMU's Transformer-XL
    - XLNet 
    - Facebook's XLM
    

    * and a total of 27 pretrained model weights for these architectures.



2) https://github.com/hundredblocks/concrete_NLP_tutorial/blob/master/NLP_notebook.ipynb


3) https://www.vice.com/en_us/article/neagpb/ai-trained-on-old-scientific-papers-makes-discoveries-humans-missed

4) https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/discussion/91432#latest-565895

5) https://medium.com/dair-ai/pytorch-transformers-for-state-of-the-art-nlp-3348911ffa5b

6) https://wit.ai/
   https://github.com/facebook/duckling

7) https://github.com/facebookresearch/pytext

