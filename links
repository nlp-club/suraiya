1) https://github.com/huggingface/pytorch-transformers 

Summary of the above as found on Reddit:

The library is comprised of six architectures:


    - Google's BERT,
    - OpenAI's GPT,
    - OpenAI's GPT-2,
    - Google/CMU's Transformer-XL
    - XLNet 
    - Facebook's XLM
    

    * and a total of 27 pretrained model weights for these architectures.



2) https://github.com/hundredblocks/concrete_NLP_tutorial/blob/master/NLP_notebook.ipynb


3) https://www.vice.com/en_us/article/neagpb/ai-trained-on-old-scientific-papers-makes-discoveries-humans-missed

